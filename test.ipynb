{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c135c9-ab34-4013-b834-0710c983d52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650fd786-dbcd-4a72-80ce-fe934f5fc103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# detect_face，detect face======\n",
    "def detect_face(img_cv):\n",
    "    face_model_path = 'yolov8n-face.pt'  # 人脸检测模型\n",
    "    face_model = YOLO(face_model_path, task='detect')   # 人脸检测模型\n",
    "    results = face_model(img_cv)    # 检测图片中的人脸\n",
    "    res = results[0].boxes.xyxy.tolist()   # 提取检测结果，可能有多张人脸\n",
    "    num = len(res)   # 人脸数量\n",
    "    print(f'检测到了{num}张人脸')\n",
    "    return res\n",
    "\n",
    "# img_path = \"data/1.jpeg\"  # 需要检测的图片地址\n",
    "# res = detect_face(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6450061c-630f-45f8-a83f-adfb7a316f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 函数check_face()\n",
    "def check_face(res,img_original):\n",
    "    emotion_label_dict = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}\n",
    "    expression_model_path = r'expression_class_yolov8.pt'  # 训练的表情分类模型\n",
    "    expression_model = YOLO(expression_model_path, task='detect')  # 表情识别模型\n",
    "    tmp = []  # 保存识别结果,[表情，x1,y1,x2,y2]\n",
    "    for each in res:  # 遍历多张人脸\n",
    "        x1,y1,x2,y2 = each[:4]\n",
    "        x1 = int(x1)  # 左\n",
    "        y1 = int(y1)  # 上\n",
    "        x2 = int(x2)  # 右\n",
    "        y2 = int(y2)  # 下\n",
    "        # 开始的y坐标:结束的y坐标,开始x:结束的x\n",
    "        img_face = img_original[y1:y2,x1:x2]  # 抠出人脸\n",
    "        results = expression_model(img_face)  # 检测抠出的人脸\n",
    "        result = results[0]\n",
    "        boxes = result.boxes\n",
    "        if boxes:\n",
    "            # 用于在图像上绘制矩形，通过指定矩形的顶点坐标和颜色，可以在图像上标注感兴趣的区域。\n",
    "            emotion = int(boxes.cls.numpy()[0])\n",
    "            label = emotion_label_dict[emotion]\n",
    "            tmp.append([label,x1,y1,x2,y2])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f3f8d9-7ce3-4979-87c4-385fc9429d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 16 faces, 6.0ms\n",
      "Speed: 6.1ms preprocess, 6.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "WARNING ⚠️ expression_class_yolov8.pt appears to require 'ultralytics.utils', which is not in ultralytics requirements.\n",
      "AutoInstall will run now for 'ultralytics.utils' but this feature will be removed in the future.\n",
      "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到了16张人脸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['ultralytics.utils'] not found, attempting AutoUpdate...\n",
      "ERROR: Could not find a version that satisfies the requirement ultralytics.utils (from versions: none)\n",
      "ERROR: No matching distribution found for ultralytics.utils\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache \"ultralytics.utils\"  ' returned non-zero exit status 1.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/autodl-fs/data/YOLO_FER/yolov8-face-main/ultralytics/nn/tasks.py:519\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, file  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1165\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1164\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.utils'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m img_original\u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)     \n\u001b[1;32m      3\u001b[0m res \u001b[38;5;241m=\u001b[39m detect_face(img_original)\n\u001b[0;32m----> 4\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_original\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label,x1,y1,x2,y2 \u001b[38;5;129;01min\u001b[39;00m tmp:              \n\u001b[1;32m      6\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(img_original, (x1,y1), (x2,y2), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mcheck_face\u001b[0;34m(res, img_original)\u001b[0m\n\u001b[1;32m      3\u001b[0m emotion_label_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manger\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisgust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhappiness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msadness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurprise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m6\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      4\u001b[0m expression_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpression_class_yolov8.pt\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 训练的表情分类模型\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m expression_model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 表情识别模型\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tmp \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# 保存识别结果,[表情，x1,y1,x2,y2]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m res:  \u001b[38;5;66;03m# 遍历多张人脸\u001b[39;00m\n",
      "File \u001b[0;32m/autodl-fs/data/YOLO_FER/yolov8-face-main/ultralytics/yolo/engine/model.py:107\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/autodl-fs/data/YOLO_FER/yolov8-face-main/ultralytics/yolo/engine/model.py:156\u001b[0m, in \u001b[0;36mYOLO._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    154\u001b[0m suffix \u001b[38;5;241m=\u001b[39m Path(weights)\u001b[38;5;241m.\u001b[39msuffix\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m/autodl-fs/data/YOLO_FER/yolov8-face-main/ultralytics/nn/tasks.py:579\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    578\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m/autodl-fs/data/YOLO_FER/yolov8-face-main/ultralytics/nn/tasks.py:534\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    528\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING ⚠️ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m appears to require \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which is not in ultralytics requirements.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAutoInstall will run now for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but this feature will be removed in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecommend fixes are to train a new model using the latest \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package or to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun a command with an official YOLOv8 model, i.e. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo predict model=yolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    532\u001b[0m check_requirements(e\u001b[38;5;241m.\u001b[39mname)  \u001b[38;5;66;03m# install missing module\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, file\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1165\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.utils'"
     ]
    }
   ],
   "source": [
    "img_path = 'data/1.jpeg'  # 图片\n",
    "img_original= cv2.imread(img_path)     \n",
    "res = detect_face(img_original)\n",
    "tmp = check_face(res,img_original)\n",
    "for label,x1,y1,x2,y2 in tmp:              \n",
    "    cv2.rectangle(img_original, (x1,y1), (x2,y2), (0, 255, 0), 1)\n",
    "    cv2.putText(img_original, label, ((x1, y1 +20)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 250),1, cv2.LINE_AA)\n",
    "# cv2.imshow('Gesture Recognition', img_original)  # 显示结果\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "cv2.imwrite('image1.jpg', img_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6deb05-95da-4249-b3be-21d17f7986ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64554024-5014-4d35-9a7d-15f255cb57fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153893f9-efd9-4299-8b4d-af9c16909b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5954d27-03ae-444e-a90a-427ce2e04508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Ultralytics settings reset to defaults. This is normal and may be due to a recent ultralytics package update, but may have overwritten previous settings. \n",
      "View and update settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.yaml'\n",
      "\n",
      "image 1/1 /autodl-fs/data/YOLO_FER/yolov8-face-main/data/1.jpeg: 640x640 16 faces, 6.4ms\n",
      "Speed: 2.9ms preprocess, 6.4ms inference, 113.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到了16张人脸\n",
      "[[403.0, 21.0, 472.0, 116.0], [156.0, 270.0, 227.0, 367.0], [402.0, 271.0, 472.0, 366.0], [32.0, 269.0, 104.0, 366.0], [279.0, 149.0, 349.0, 241.0], [30.0, 23.0, 98.0, 112.0], [156.0, 22.0, 227.0, 116.0], [278.0, 398.0, 348.0, 489.0], [402.0, 398.0, 472.0, 490.0], [281.0, 21.0, 350.0, 114.0], [29.0, 146.0, 96.0, 237.0], [402.0, 143.0, 466.0, 240.0], [30.0, 397.0, 95.0, 486.0], [280.0, 273.0, 346.0, 361.0], [152.0, 395.0, 223.0, 485.0], [153.0, 142.0, 226.0, 232.0]]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "path = 'yolov8n-face.pt'  # 人脸检测模型\n",
    "img_path = \"data/1.jpeg\"  # 需要检测的图片地址\n",
    "model = YOLO(path, task='detect')\n",
    "results = model(img_path)  # 检测图片\n",
    "res = results[0].boxes.xyxy.tolist() # 提取检测结果，可能有多张人脸\n",
    "num = len(res)   # 人脸数量\n",
    "print(f'检测到了{num}张人脸')\n",
    "print(res)\n",
    "# img = cv2.imread(img_path)\n",
    "# for each in res: # 可能有多张人脸\n",
    "#     x1,y1,x2,y2 = each[:4]\n",
    "#     x1 = int(x1)\n",
    "#     y1 = int(y1)\n",
    "#     x2 = int(x2)\n",
    "#     y2 = int(y2)\n",
    "#     cv2.rectangle(img, (x1, y1), (x2, y2), (50, 50, 250), 3)\n",
    "# cv2.imshow('face_detection', img)\n",
    "# cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e272213-d1a9-4d1f-ae38-d14baad5b47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961af59-be6a-43bf-b988-aea7e6c4aae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf1246-4076-423f-9661-59a4a1cca838",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'dgjq.jpg'  # 图片\n",
    "img_original= cv2.imread(img_path)     \n",
    "res = detect_face(img_original)\n",
    "tmp = check_face(res,img_original)\n",
    "for label,x1,y1,x2,y2 in tmp:              \n",
    "    cv2.rectangle(img_original, (x1,y1), (x2,y2), (0, 255, 0), 1)\n",
    "    cv2.putText(img_original, label, ((x1, y1 +20)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 250),1, cv2.LINE_AA)\n",
    "cv2.imshow('Gesture Recognition', img_original)  # 显示结果\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045748d8-05ef-48c0-830b-0b8c3fdbe699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27ef51-abbd-49ba-be2a-63cc52933442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9068d-2983-407e-9153-43a68c0ea1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6dd5a-b9df-4e77-a8cb-cdce599f2d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b0bb5b-4283-4d21-9c89-b859e0831088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /autodl-fs/data/YOLO_FER/yolov8-face-main/data/1.jpeg: 640x640 16 faces, 7.5ms\n",
      "Speed: 2.8ms preprocess, 7.5ms inference, 125.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到了16张人脸\n"
     ]
    }
   ],
   "source": [
    "# 函数detect_face，检测人脸\n",
    "def detect_face(img_cv):\n",
    "    face_model_path = 'yolov8n-face.pt'  # 人脸检测模型\n",
    "    face_model = YOLO(face_model_path, task='detect')   # 人脸检测模型\n",
    "    results = face_model(img_cv)    # 检测图片中的人脸\n",
    "    res = results[0].boxes.xyxy.tolist()   # 提取检测结果，可能有多张人脸\n",
    "    num = len(res)   # 人脸数量\n",
    "    print(f'检测到了{num}张人脸')\n",
    "    return res\n",
    "\n",
    "img_path = \"data/1.jpeg\"  # 需要检测的图片地址\n",
    "res = detect_face(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28f9af-c70a-449d-952c-06c664d3b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数check_face()\n",
    "def check_face(res,img_original):\n",
    "    emotion_label_dict = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}\n",
    "    expression_model_path = r'runs\\detect\\train10\\weights\\best.pt'  # 训练的表情分类模型\n",
    "    expression_model = YOLO(expression_model_path)  # 表情识别模型\n",
    "    tmp = []  # 保存识别结果,[表情，x1,y1,x2,y2]\n",
    "    for each in res:  # 遍历多张人脸\n",
    "        x1,y1,x2,y2 = each[:4]\n",
    "        x1 = int(x1)  # 左\n",
    "        y1 = int(y1)  # 上\n",
    "        x2 = int(x2)  # 右\n",
    "        y2 = int(y2)  # 下\n",
    "        # 开始的y坐标:结束的y坐标,开始x:结束的x\n",
    "        img_face = img_original[y1:y2,x1:x2]  # 抠出人脸\n",
    "        results = expression_model(img_face)  # 检测抠出的人脸\n",
    "        result = results[0]\n",
    "        boxes = result.boxes\n",
    "        if boxes:\n",
    "            # 用于在图像上绘制矩形，通过指定矩形的顶点坐标和颜色，可以在图像上标注感兴趣的区域。\n",
    "            emotion = int(boxes.cls.numpy()[0])\n",
    "            label = emotion_label_dict[emotion]\n",
    "            tmp.append([label,x1,y1,x2,y2])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a3a7f-1d7e-4d79-a418-db728c76daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 多张人脸测试\n",
    "img_path = 'dgjq.jpg'  # 图片\n",
    "img_original= cv2.imread(img_path)\n",
    "res = detect_face(img_original)\n",
    "tmp = check_face(res,img_original)\n",
    "for label,x1,y1,x2,y2 in tmp:\n",
    "    cv2.rectangle(img_original, (x1,y1), (x2,y2), (0, 255, 0), 1)\n",
    "    cv2.putText(img_original, label, ((x1, y1 +20)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 250),1, cv2.LINE_AA)\n",
    "cv2.imshow('Gesture Recognition', img_original)  # 显示结果\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
